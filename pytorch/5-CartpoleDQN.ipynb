{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym as gym\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "import cv2 as cv2\n",
    "import random\n",
    "# %matplotlib inline\n",
    "# matplotlib.get_backend()\n",
    "# plt.isinteractive()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdb978d0710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    screen = env.render(mode='rgb_array')\n",
    "    plt.imshow(screen)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "display.clear_output(wait=True)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-14 23:12:55,641] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 600)\n"
     ]
    }
   ],
   "source": [
    "def process_screen(image):\n",
    "    img = image[150:330, :]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    return img\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "prev_screen = process_screen(env.render(mode='rgb_array'))\n",
    "\n",
    "for i in range(1):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    screen = env.render(mode='rgb_array')\n",
    "    screen = process_screen(screen)\n",
    "    print(screen.shape)\n",
    "    plt.imshow(screen, cmap=\"gray\")\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 600)\n",
      "(180, 600, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdb93e24fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(prev_screen.shape)\n",
    "out = np.stack([screen, prev_screen, np.zeros_like(screen)], axis=2)\n",
    "print(out.shape)\n",
    "plt.imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition(s=[0, 1, 2, 3], a=0, s_1=[4, 5, 6, 7], r=0)\n"
     ]
    }
   ],
   "source": [
    "Transition = namedtuple(\"Transition\", [\"s\", \"a\", \"s_1\", \"r\"])\n",
    "\n",
    "a = Transition([0, 1, 2, 3], 0, [4, 5, 6, 7], 0)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([1, 2, 3, 4, 5], maxlen=5)\n",
      "deque([2, 3, 4], maxlen=3)\n"
     ]
    }
   ],
   "source": [
    "deck = deque(maxlen=5)\n",
    "\n",
    "k = 0\n",
    "for i in range(6):\n",
    "    deck.append(k)\n",
    "    k += 1\n",
    "\n",
    "print(deck)\n",
    "\n",
    "deck = deque(range(5), maxlen=3)\n",
    "print(deck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len:  4\n",
      "Memory: 4 \n",
      "5 \n",
      "6 \n",
      "3 \n",
      "\n",
      "[array([[0, 1, 2, 3],\n",
      "       [0, 1, 2, 3]]), array([[0],\n",
      "       [0]]), array([[4, 5, 6, 7],\n",
      "       [4, 5, 6, 7]]), array([[0],\n",
      "       [0]])]\n",
      "(2, 4)\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, item):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = item\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        out = random.sample(self.memory, batch_size)\n",
    "        batched = Transition(*zip(*out))\n",
    "        s = np.array(list(batched.s))\n",
    "        a = np.expand_dims(np.array(list(batched.a)), axis=1)\n",
    "        s_1 = np.array(list(batched.s_1))\n",
    "        r = np.expand_dims(np.array(list(batched.r)), axis=1)\n",
    "        return [s, a, s_1, r]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def __str__(self):\n",
    "        result = []\n",
    "        for i in range(self.__len__()):\n",
    "            result.append(self.memory[i].__str__() + \" \\n\")\n",
    "        return \"\".join(result)\n",
    "        \n",
    "\n",
    "memory = ReplayMemory(capacity=4)\n",
    "\n",
    "for i in range(7):\n",
    "    memory.push(i)\n",
    "\n",
    "\n",
    "print(\"len: \", len(memory))\n",
    "print(\"Memory:\", memory)\n",
    "\n",
    "memory = ReplayMemory(capacity=10)\n",
    "\n",
    "for i in range(10):\n",
    "    a = Transition([0, 1, 2, 3], 0, [4, 5, 6, 7], 0)\n",
    "    memory.push(a)\n",
    "    \n",
    "sample = memory.sample(2)\n",
    "print(sample)\n",
    "print(sample[0].shape)\n",
    "print(sample[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQNLinear (\n",
      "  (fc1): Linear (4 -> 10)\n",
      "  (fc2): Linear (10 -> 10)\n",
      "  (fc3): Linear (10 -> 2)\n",
      ")\n",
      "(1, 4)\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "class DQNLinear(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(DQNLinear, self).__init__(*args)\n",
    "        self.fc1 = nn.Linear(4, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "dqn = DQNLinear()\n",
    "print(dqn)\n",
    "\n",
    "# x = np.stack([screen, prev_screen], axis=2)\n",
    "# x = np.expand_dims(x, axis=0)\n",
    "# x = np.rollaxis(x, 3, 1)\n",
    "x = torch.Tensor([1, 1, 1, 1])\n",
    "x = torch.unsqueeze(x, 0)\n",
    "x = Variable(x).float()\n",
    "print(x.data.numpy().shape)\n",
    "# print(x)\n",
    "# x = Variable(torch.randn(2, 2, 180, 600))\n",
    "        \n",
    "output = dqn(x)\n",
    "print(output.data.numpy().shape)\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(dqn.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = dqn(x)\n",
    "loss = criterion(output, Variable(torch.Tensor([0,0])))\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok, now we do DQN with the continuous states received rather than pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.99\n",
      "0.98\n",
      "0.97\n",
      "0.01\n",
      "0.0\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "class Epsilon(object):\n",
    "    def __init__(self, start=1.0, end=0.01, update_increment=0.01):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.update_increment = update_increment\n",
    "        self._value = self.start\n",
    "        self.isTraining = True\n",
    "    \n",
    "    def increment(self, count=1):\n",
    "        self._value = max(self.end, self._value - self.update_increment*count)\n",
    "        return self\n",
    "        \n",
    "    def value(self):\n",
    "        if not self.isTraining:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return self._value\n",
    "    \n",
    "    \n",
    "eps = Epsilon(start=1.0, end=0.01, update_increment=0.01)\n",
    "print(eps.value())\n",
    "print(eps.increment().value())\n",
    "print(eps.increment().value())\n",
    "print(eps.increment().value())\n",
    "print(eps.increment(99).value())\n",
    "eps.isTraining = False\n",
    "print(eps.increment().value())\n",
    "eps.isTraining = True\n",
    "print(eps.increment().value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-15 00:16:40,877] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training q\n",
      "Variable containing:\n",
      " 0.4047  0.2223\n",
      " 0.4110  0.2460\n",
      " 0.4167  0.2441\n",
      " 0.4114  0.2261\n",
      "[torch.FloatTensor of size 4x2]\n",
      "\n",
      "-----------\n",
      "(Variable containing:\n",
      " 0.4047\n",
      " 0.4110\n",
      " 0.4167\n",
      " 0.4114\n",
      "[torch.FloatTensor of size 4]\n",
      ", Variable containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.LongTensor of size 4]\n",
      ")\n",
      "Episode reward:  15.0\n"
     ]
    }
   ],
   "source": [
    "class DQNLinearLearner(object):\n",
    "    def __init__(self, env=None):\n",
    "        self.env = env\n",
    "        self.epsilon = Epsilon(start=1.0, end=0.01, update_increment=0.01)\n",
    "        self.gamma = 0.99\n",
    "        self.train_q_per_step = 10\n",
    "        self.train_q_batch_size = 10\n",
    "        \n",
    "        self.memory = ReplayMemory(capacity=1000)\n",
    "        self.dqn = DQNLinear()\n",
    "        self.reset()\n",
    "        \n",
    "\n",
    "    def get_action(self, s):\n",
    "        s = self.state_to_tensor(s)\n",
    "        actions = self.dqn(s)\n",
    "        if np.random.rand() > self.epsilon.value():\n",
    "            action = np.argmax(actions.data.numpy())\n",
    "#             print(actions.data.numpy())\n",
    "#             print(\"not exploring\", self.epsilon.value())\n",
    "        else:\n",
    "            action = self.env.action_space.sample()\n",
    "#             print(\"exploring\", action, \"epsilon:\", self.epsilon.value())\n",
    "        return action\n",
    "\n",
    "    def state_to_tensor(self, s):\n",
    "        x = torch.Tensor(s)\n",
    "        x = torch.unsqueeze(x, 0)\n",
    "        x = Variable(x).float()\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.s = env.reset()\n",
    "    \n",
    "    def train(self, nb_episodes=1):\n",
    "        self.epsilon.isTraining = True\n",
    "        step = 0\n",
    "        \n",
    "        for episode in range(nb_episodes):\n",
    "            self.reset()\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while True:\n",
    "                action = self.get_action(self.s)\n",
    "                s_1, r, done, _ = self.env.step(action)\n",
    "                if done:\n",
    "                    r = 0\n",
    "                    \n",
    "                transition = Transition(self.s, action, s_1, r)\n",
    "                self.memory.push(transition)\n",
    "                episode_reward += r\n",
    "                step += 1\n",
    "                \n",
    "                if done:\n",
    "                    break;\n",
    "                \n",
    "                if step % self.train_q_per_step == 0 and step > 0:\n",
    "                    self.train_q()\n",
    "                \n",
    "            self.epsilon.increment() # increment epsilon per episode\n",
    "            print(\"Episode reward: \", episode_reward)\n",
    "    \n",
    "    def train_q(self):\n",
    "        print(\"training q\")\n",
    "        if self.train_q_batch_size < len(self.memory):\n",
    "            return\n",
    "        s, a, s_1, r = self.memory.sample(4)\n",
    "        s = Variable(torch.from_numpy(s)).float()\n",
    "        a = Variable(torch.from_numpy(a)).float()\n",
    "        s_1 = Variable(torch.from_numpy(s_1)).float()\n",
    "        r = Variable(torch.from_numpy(r)).float()\n",
    "        \n",
    "        # Q_sa = r + gamma * max(Q_s'a')\n",
    "        left = self.dqn(s)  #Q_sa\n",
    "        right = self.dqn(s_1)\n",
    "        \n",
    "        print(right)\n",
    "        print(\"-----------\")\n",
    "        print(right.max(1))\n",
    "\n",
    "    def run(self):\n",
    "        self.reset()\n",
    "        self.epsilon.isTraining = False\n",
    "        self.env.render()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            self.env.render()\n",
    "            action = self.get_action(self.s)\n",
    "            s_1, r, done, _ = self.env.step(action)\n",
    "            episode_reward += r\n",
    "\n",
    "            if done:\n",
    "                break;\n",
    "        \n",
    "        self.env.render(close=True)\n",
    "        print(\"Total Reward: \", episode_reward)\n",
    "    \n",
    "env = gym.make(\"CartPole-v0\")\n",
    "learner = DQNLinearLearner(env)\n",
    "\n",
    "learner.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_screen(image):\n",
    "    img = image[150:330, :]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    return img\n",
    "\n",
    "def image2state(s1, s0=None):\n",
    "    img1 = process_screen(s1)\n",
    "    if s0 is not None:\n",
    "        img0 = process_screen(s0)\n",
    "    else:\n",
    "        img0 = np.zeros_like(img1)\n",
    "        \n",
    "    out = np.stack([img1, img0], axis=2)\n",
    "    out = np.rollaxis(out, 2, 0)\n",
    "    out = Variable(torch.from_numpy(out)).float()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(DQN, self).__init__(*args)\n",
    "        self.conv1 = nn.Conv2d(2, 10, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(10, 15, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(15, 20, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(20, 25, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(25, 30, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(30 * 6 * 19, 600)\n",
    "        self.fc2 = nn.Linear(600, 100)\n",
    "        self.fc3 = nn.Linear(100, 20)\n",
    "        self.fc4 = nn.Linear(20, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "#         print(x.view(x.size(0), -1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "dqn = DQN()\n",
    "print(dqn)\n",
    "\n",
    "x = np.stack([screen, prev_screen], axis=2)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = np.rollaxis(x, 3, 1)\n",
    "x = Variable(torch.from_numpy(x)).float()\n",
    "print(x.data.numpy().shape)\n",
    "# print(x)\n",
    "# x = Variable(torch.randn(2, 2, 180, 600))\n",
    "        \n",
    "output = dqn(x)\n",
    "print(output.data.numpy().shape)\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(dqn.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = dqn(x)\n",
    "loss = criterion(output, Variable(torch.Tensor([0,0])))\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
